{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "enable_visualization = True\n",
    "if enable_visualization:\n",
    "    env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86_64')\n",
    "else:\n",
    "    env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in  1000  steps\n",
      "Total score (averaged over agents) this episode: 0.1904999957419932\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "t_max = 10000\n",
    "for t in range(t_max):\n",
    "# while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    # print('current time: ', t)\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        print('done in ', t, ' steps')\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from ddpg_agent import Agent\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "# widget bar to display progress\n",
    "# !pip install progressbar\n",
    "import progressbar as pb\n",
    "\n",
    "\n",
    "# Methods on how to improve performance\n",
    "# 1. Deeper and more complex networks\n",
    "#   1.1 make critic deeper(ddpg_result_2022_09_30_15_35_43), batch(40): not working, reward stay below 1\n",
    "# 2. Delayed updates\n",
    "# 3. Tuned hyper parameters\n",
    "#   3.1 different batch size\n",
    "#   20(ddpg_result_2022_09_30_13_04_29)/80(ddpg_result_2022_09_30_14_24_26) not good, 40 good(ddpg_result_2022_09_30_13_27_48)\n",
    "# 4. Noise function (uniform to normal distribution)\n",
    "#   4.1 same batch size(40), smaller variation, better performance(ddpg_result_2022_09_30_13_27_48)\n",
    "# 5. BatchNormalization\n",
    "# 6. Dueling Q network(Not applicable for ddpg continuous control)\n",
    "# 7. N-step bootstrapping\n",
    "# 8. Importance Sampling Priority Queue\n",
    "\n",
    "# Try 1\n",
    "# UPDATE_STEP: 1->20, UPDATE_TIMES: 1->10, BATCH_SIZE: 40(ddpg_result_2022_09_30_16_08_56): not working\n",
    "# Try 2: Change multi_step to step\n",
    "# UPDATE_STEP: 20, UPDATE_TIMES: 5, BATCH_SIZE: 40(ddpg_result_2022_09_30_16_49_02): not working\n",
    "# Try 3:\n",
    "# UPDATE_STEP: 20, UPDATE_TIMES: 5, BATCH_SIZE: 40, disable grad clip for critic, change update time logic: better(ddpg_result_2022_09_30_17_28_04)\n",
    "# Try 4:\n",
    "# UPDATE_STEP: 20, UPDATE_TIMES: 5, BATCH_SIZE: 40, above and add BN in both net:(ddpg_result_2022_10_01_15_16_22) better but still fails\n",
    "# Try 5:\n",
    "# UPDATE_STEP: 20, UPDATE_TIMES: 5, BATCH_SIZE: 128, above:(ddpg_result_2022_10_01_19_12_28) much better but still fails\n",
    "# Try 6:\n",
    "# above and change net param(400,300)->(128,256):(ddpg_result_2022_10_01_22_51_33) worse\n",
    "# Try 7:\n",
    "# same as 5, but increase buffer 1e5->1e6(ddpg_result_2022_10_02_01_44_19): much better but not enough\n",
    "# Try 8:\n",
    "# same as above but change step logic(ddpg_result_2022_10_02_10_04_21): not working\n",
    "# Try 9:\n",
    "# same as above but change noise from normal to ou, noise std 0.2->0.05(ddpg_result_2022_10_02_12_18_34): much better but not enough\n",
    "# Try 10:\n",
    "# same as above but increase update times 5->7:(ddpg_result_2022_10_02_18_35_44) worse\n",
    "# Try 11:\n",
    "# same as above and change update time logic back:(ddpg_result_2022_10_02_22_45_19) huge bump ast start, but then not growing\n",
    "# Try 12:\n",
    "# same as above and change GAMMA 0.99->0.95:(ddpg_result_2022_10_02_23_57_47) not working\n",
    "# Try 13:\n",
    "# fix ou noise error by adding -0.5 bias and add reset:(ddpg_result_2022_10_03_02_05_59) much better but not enough\n",
    "# Try 14:\n",
    "# change detach to no_grad: (ddpg_result_2022_10_04_16_14_37) success\n",
    "# Try 13:\n",
    "# add prioritized exp replay, change update time to 1: (ddpg_result_2022_10_08_21_09_12) success and much faster(however the env unity env got stuck at communication, so only finished 44 episodes)\n",
    "\n",
    "\n",
    "\n",
    "# conclusion\n",
    "# small sampler first to evaluate the learning ability\n",
    "# then increase batch size to give more data to learn\n",
    "\n",
    "def plot_result(scores, scores_avg, actual_target_score, save_path):\n",
    "    target_score_curve = np.ones(len(scores)) * actual_target_score\n",
    "    fig = plt.figure(figsize=[15,10])\n",
    "    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    plt.xlabel(\"episode\"), plt.ylabel(\"score\")\n",
    "    ax.plot(scores)\n",
    "    ax.plot(scores_avg)\n",
    "    ax.plot(target_score_curve)\n",
    "    ax.legend(['score','avg_score', 'target_score'])\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "def Learn(env: UnityEnvironment, n_episodes=1000, max_t=1000, target_score=0.0, actual_target_score=0.0, prioritized_learn=False):\n",
    "\n",
    "    cur_folder = os.getcwd()\n",
    "    str_time = datetime.datetime.strftime(datetime.datetime.now(), '%Y_%m_%d_%H_%M_%S')\n",
    "    folder_name = '/ddpg_result_' + str_time + '/'\n",
    "    save_path = cur_folder + folder_name\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "\n",
    "    scores_window = deque(maxlen=20)\n",
    "    avg_agent_scores = []\n",
    "    avg_scores = []\n",
    "    cur_target_score = target_score  \n",
    "\n",
    "    agent = Agent(env, save_path+'log.txt')\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        agent.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        num_agents = len(env_info.agents)\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        for t in range(max_t):\n",
    "            actions = agent.multi_agent_act(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            # agent.multi_agent_step(states, actions, rewards, next_states, dones)\n",
    "            for i in range(num_agents):\n",
    "                agent.priority_step(states[i], actions[i], rewards[i], next_states[i], dones[i], t)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            # print('current time: ', t)\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        scores_window.append(np.mean(scores))\n",
    "        print('\\nEpisode {}\\tTotal score (averaged over agents) this episode: {}'.format(i_episode, scores_window[-1]))\n",
    "        avg_agent_scores.append(scores_window[-1])\n",
    "        avg_scores.append(np.mean(scores_window))\n",
    "        \n",
    "        # update progress widget bar\n",
    "        timer.update(i_episode)\n",
    "        \n",
    "        if avg_agent_scores[-1] > cur_target_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, avg_scores[-1]))\n",
    "            result_str = 'epi_' + str(i_episode) + '_score_' + str(avg_scores[-1])\n",
    "            torch.save(agent.critic_local.state_dict(), save_path+result_str+'_critic_checkpoint.pth')\n",
    "            torch.save(agent.actor_local.state_dict(), save_path+result_str+'_actor_checkpoint.pth')\n",
    "            cur_target_score = avg_agent_scores[-1]\n",
    "        \n",
    "        pic_save_path = save_path+'results/'\n",
    "        if not os.path.isdir(pic_save_path):\n",
    "            os.mkdir(pic_save_path)\n",
    "        pic_name = 'ddpg_n_epi_'+str(i_episode)+'.png'    \n",
    "        plot_result(avg_agent_scores, avg_scores, actual_target_score, pic_save_path+pic_name) \n",
    "        \n",
    "        if i_episode > 100 and avg_scores[-1] > actual_target_score:\n",
    "            print('training done')\n",
    "            break\n",
    "    \n",
    "    timer.finish()\n",
    "\n",
    "    return avg_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 33 action size: 4\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.010 \tlearn_time: 0.028 \tsum: 117.927 \tsize: 20015                                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  13:52:24\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.008 \tlearn_time: 0.035 \tsum: 118.894 \tsize: 20020                        \n",
      "Episode 1\tTotal score (averaged over agents) this episode: 0.8384999812580645\n",
      "\n",
      "Environment solved in 1 episodes!\tAverage Score: 0.84\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.011 \tlearn_time: 0.072 \tsum: 279.658 \tsize: 40039                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  14:06:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.011 \tlearn_time: 0.073 \tsum: 278.829 \tsize: 40040                        \n",
      "Episode 2\tTotal score (averaged over agents) this episode: 0.7954999822191894\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.004 \tupdate_time: 0.010 \tlearn_time: 0.035 \tsum: 554.480 \tsize: 60053                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   1% |                                          | ETA:  14:24:16\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.008 \tlearn_time: 0.015 \tsum: 558.999 \tsize: 60060                        \n",
      "Episode 3\tTotal score (averaged over agents) this episode: 1.436999967880547\n",
      "\n",
      "Environment solved in 3 episodes!\tAverage Score: 1.02\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.011 \tlearn_time: 0.051 \tsum: 961.995 \tsize: 80079                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   1% |                                          | ETA:  14:19:36\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.009 \tupdate_time: 0.013 \tlearn_time: 0.057 \tsum: 959.036 \tsize: 80080                        \n",
      "Episode 4\tTotal score (averaged over agents) this episode: 1.7029999619349838\n",
      "\n",
      "Environment solved in 4 episodes!\tAverage Score: 1.19\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.008 \tlearn_time: 0.018 \tsum: 1394.153 \tsize: 100097                                                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   1% |                                          | ETA:  14:49:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.011 \tupdate_time: 0.009 \tlearn_time: 0.024 \tsum: 1383.382 \tsize: 100100                        \n",
      "Episode 5\tTotal score (averaged over agents) this episode: 1.9174999571405351\n",
      "\n",
      "Environment solved in 5 episodes!\tAverage Score: 1.34\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.004 \tupdate_time: 0.008 \tlearn_time: 0.031 \tsum: 1933.352 \tsize: 120114                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |                                          | ETA:  13:59:16\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.008 \tlearn_time: 0.015 \tsum: 1921.081 \tsize: 120120                        \n",
      "Episode 6\tTotal score (averaged over agents) this episode: 2.7384999387897553\n",
      "\n",
      "Environment solved in 6 episodes!\tAverage Score: 1.57\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.004 \tupdate_time: 0.008 \tlearn_time: 0.017 \tsum: 2463.224 \tsize: 140133                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |                                          | ETA:  13:22:42\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.009 \tlearn_time: 0.015 \tsum: 2449.798 \tsize: 140140                        \n",
      "Episode 7\tTotal score (averaged over agents) this episode: 2.966499933693558\n",
      "\n",
      "Environment solved in 7 episodes!\tAverage Score: 1.77\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.010 \tupdate_time: 0.009 \tlearn_time: 0.024 \tsum: 3080.704 \tsize: 160159                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |#                                         | ETA:  12:51:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.009 \tlearn_time: 0.041 \tsum: 3079.261 \tsize: 160160                        \n",
      "Episode 8\tTotal score (averaged over agents) this episode: 3.896499912906438\n",
      "\n",
      "Environment solved in 8 episodes!\tAverage Score: 2.04\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.009 \tlearn_time: 0.016 \tsum: 3738.242 \tsize: 180179                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                         | ETA:  12:30:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.009 \tlearn_time: 0.055 \tsum: 3738.646 \tsize: 180180                        \n",
      "Episode 9\tTotal score (averaged over agents) this episode: 4.479999899864197\n",
      "\n",
      "Environment solved in 9 episodes!\tAverage Score: 2.31\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.010 \tlearn_time: 0.021 \tsum: 4416.432 \tsize: 200197                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                         | ETA:  12:10:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.009 \tlearn_time: 0.016 \tsum: 4409.892 \tsize: 200200                        \n",
      "Episode 10\tTotal score (averaged over agents) this episode: 5.843499869387597\n",
      "\n",
      "Environment solved in 10 episodes!\tAverage Score: 2.66\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.009 \tlearn_time: 0.020 \tsum: 5152.131 \tsize: 220216                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                         | ETA:  11:57:44\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.010 \tlearn_time: 0.018 \tsum: 5137.149 \tsize: 220220                        \n",
      "Episode 11\tTotal score (averaged over agents) this episode: 7.239999838173389\n",
      "\n",
      "Environment solved in 11 episodes!\tAverage Score: 3.08\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.009 \tlearn_time: 0.018 \tsum: 6000.621 \tsize: 240233                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                         | ETA:  11:44:41\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.010 \tlearn_time: 0.066 \tsum: 5981.068 \tsize: 240240                        \n",
      "Episode 12\tTotal score (averaged over agents) this episode: 8.04299982022494\n",
      "\n",
      "Environment solved in 12 episodes!\tAverage Score: 3.49\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.012 \tlearn_time: 0.080 \tsum: 6889.959 \tsize: 260257                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                         | ETA:  11:34:07\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.009 \tlearn_time: 0.068 \tsum: 6879.609 \tsize: 260260                        \n",
      "Episode 13\tTotal score (averaged over agents) this episode: 9.12549979602918\n",
      "\n",
      "Environment solved in 13 episodes!\tAverage Score: 3.92\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.011 \tlearn_time: 0.026 \tsum: 7892.099 \tsize: 280279                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                         | ETA:  11:26:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.012 \tlearn_time: 0.064 \tsum: 7888.904 \tsize: 280280                        \n",
      "Episode 14\tTotal score (averaged over agents) this episode: 11.870999734662472\n",
      "\n",
      "Environment solved in 14 episodes!\tAverage Score: 4.49\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.011 \tlearn_time: 0.019 \tsum: 8931.242 \tsize: 300299                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   5% |##                                        | ETA:  11:19:47\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.011 \tlearn_time: 0.024 \tsum: 8927.720 \tsize: 300300                        \n",
      "Episode 15\tTotal score (averaged over agents) this episode: 12.453999721631408\n",
      "\n",
      "Environment solved in 15 episodes!\tAverage Score: 5.02\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.013 \tlearn_time: 0.036 \tsum: 10054.294 \tsize: 320315                                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   5% |##                                        | ETA:  11:14:19\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.010 \tupdate_time: 0.012 \tlearn_time: 0.049 \tsum: 10037.126 \tsize: 320320                        \n",
      "Episode 16\tTotal score (averaged over agents) this episode: 14.608999673463405\n",
      "\n",
      "Environment solved in 16 episodes!\tAverage Score: 5.62\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.012 \tlearn_time: 0.018 \tsum: 11130.119 \tsize: 340333                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   5% |##                                        | ETA:  11:08:33\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.010 \tlearn_time: 0.031 \tsum: 11107.727 \tsize: 340340                        \n",
      "Episode 17\tTotal score (averaged over agents) this episode: 16.689499626960604\n",
      "\n",
      "Environment solved in 17 episodes!\tAverage Score: 6.27\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.012 \tlearn_time: 0.021 \tsum: 12290.192 \tsize: 360355                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                        | ETA:  11:04:48\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.010 \tlearn_time: 0.031 \tsum: 12270.920 \tsize: 360360                        \n",
      "Episode 18\tTotal score (averaged over agents) this episode: 16.8509996233508\n",
      "\n",
      "Environment solved in 18 episodes!\tAverage Score: 6.86\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.012 \tlearn_time: 0.020 \tsum: 13496.438 \tsize: 380377                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                        | ETA:  11:00:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.011 \tlearn_time: 0.018 \tsum: 13483.775 \tsize: 380380                        \n",
      "Episode 19\tTotal score (averaged over agents) this episode: 23.125499483104797\n",
      "\n",
      "Environment solved in 19 episodes!\tAverage Score: 7.72\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.010 \tlearn_time: 0.031 \tsum: 14870.324 \tsize: 400393                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                        | ETA:  10:57:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.011 \tlearn_time: 0.046 \tsum: 14841.530 \tsize: 400400                        \n",
      "Episode 20\tTotal score (averaged over agents) this episode: 23.743499469291418\n",
      "\n",
      "Environment solved in 20 episodes!\tAverage Score: 8.52\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.011 \tlearn_time: 0.034 \tsum: 16337.944 \tsize: 420417                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |##                                        | ETA:  10:54:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.011 \tlearn_time: 0.019 \tsum: 16332.023 \tsize: 420420                        \n",
      "Episode 21\tTotal score (averaged over agents) this episode: 27.731499380152673\n",
      "\n",
      "Environment solved in 21 episodes!\tAverage Score: 9.86\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.012 \tupdate_time: 0.012 \tlearn_time: 0.033 \tsum: 17786.022 \tsize: 440438                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |###                                       | ETA:  10:52:42\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.011 \tlearn_time: 0.037 \tsum: 17777.888 \tsize: 440440                        \n",
      "Episode 22\tTotal score (averaged over agents) this episode: 31.209499302413313\n",
      "\n",
      "Environment solved in 22 episodes!\tAverage Score: 11.38\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.013 \tlearn_time: 0.020 \tsum: 19243.507 \tsize: 460454                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |###                                       | ETA:  10:50:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.012 \tlearn_time: 0.041 \tsum: 19216.008 \tsize: 460460                        \n",
      "Episode 23\tTotal score (averaged over agents) this episode: 32.124999281950295\n",
      "\n",
      "Environment solved in 23 episodes!\tAverage Score: 12.92\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.012 \tlearn_time: 0.018 \tsum: 20613.672 \tsize: 480478                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                       | ETA:  10:47:59\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.014 \tlearn_time: 0.065 \tsum: 20609.262 \tsize: 480480                        \n",
      "Episode 24\tTotal score (averaged over agents) this episode: 34.851999220997094\n",
      "\n",
      "Environment solved in 24 episodes!\tAverage Score: 14.58\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.013 \tlearn_time: 0.022 \tsum: 22048.061 \tsize: 500495                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                       | ETA:  10:47:07\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.011 \tlearn_time: 0.038 \tsum: 22037.078 \tsize: 500500                        \n",
      "Episode 25\tTotal score (averaged over agents) this episode: 32.473999274149534\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.012 \tlearn_time: 0.018 \tsum: 23359.772 \tsize: 520517                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                       | ETA:  10:45:09\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.012 \tlearn_time: 0.018 \tsum: 23351.972 \tsize: 520520                        \n",
      "Episode 26\tTotal score (averaged over agents) this episode: 34.11399923749268\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.005 \tupdate_time: 0.012 \tlearn_time: 0.033 \tsum: 24643.474 \tsize: 540535                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |###                                       | ETA:  10:43:55\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.011 \tlearn_time: 0.019 \tsum: 24621.926 \tsize: 540540                        \n",
      "Episode 27\tTotal score (averaged over agents) this episode: 33.6564992477186\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.009 \tupdate_time: 0.014 \tlearn_time: 0.020 \tsum: 25862.705 \tsize: 560555                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |###                                       | ETA:  10:42:20\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.013 \tlearn_time: 0.021 \tsum: 25841.994 \tsize: 560560                        \n",
      "Episode 28\tTotal score (averaged over agents) this episode: 32.03249928401783\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.011 \tlearn_time: 0.026 \tsum: 26999.270 \tsize: 580575                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |####                                      | ETA:  10:40:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.011 \tlearn_time: 0.018 \tsum: 26972.791 \tsize: 580580                        \n",
      "Episode 29\tTotal score (averaged over agents) this episode: 33.24699925687164\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.013 \tlearn_time: 0.020 \tsum: 28274.044 \tsize: 600594                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |####                                      | ETA:  10:38:41\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.009 \tupdate_time: 0.015 \tlearn_time: 0.053 \tsum: 28252.242 \tsize: 600600                        \n",
      "Episode 30\tTotal score (averaged over agents) this episode: 33.20149925788864\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.011 \tlearn_time: 0.017 \tsum: 29391.186 \tsize: 620615                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |####                                      | ETA:  10:37:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.014 \tlearn_time: 0.052 \tsum: 29375.611 \tsize: 620620                        \n",
      "Episode 31\tTotal score (averaged over agents) this episode: 34.48199922926724\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.013 \tlearn_time: 0.020 \tsum: 30564.272 \tsize: 640639                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |####                                      | ETA:  10:35:24\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.010 \tupdate_time: 0.012 \tlearn_time: 0.022 \tsum: 30554.189 \tsize: 640640                        \n",
      "Episode 32\tTotal score (averaged over agents) this episode: 35.05699921641499\n",
      "\n",
      "Environment solved in 32 episodes!\tAverage Score: 26.13\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.012 \tlearn_time: 0.018 \tsum: 31679.900 \tsize: 660653                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  11% |####                                      | ETA:  10:33:55\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.006 \tupdate_time: 0.012 \tlearn_time: 0.020 \tsum: 31658.959 \tsize: 660660                        \n",
      "Episode 33\tTotal score (averaged over agents) this episode: 34.967999218404294\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.012 \tlearn_time: 0.033 \tsum: 33026.741 \tsize: 680673                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  11% |####                                      | ETA:  10:32:01\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.014 \tlearn_time: 0.020 \tsum: 33011.460 \tsize: 680680                        \n",
      "Episode 34\tTotal score (averaged over agents) this episode: 36.76199917830527\n",
      "\n",
      "Environment solved in 34 episodes!\tAverage Score: 28.67\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.014 \tlearn_time: 0.020 \tsum: 34142.003 \tsize: 700694                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  11% |####                                      | ETA:  10:31:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.014 \tlearn_time: 0.020 \tsum: 34117.575 \tsize: 700700                        \n",
      "Episode 35\tTotal score (averaged over agents) this episode: 37.316499165911225\n",
      "\n",
      "Environment solved in 35 episodes!\tAverage Score: 29.91\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.015 \tlearn_time: 0.052 \tsum: 35254.688 \tsize: 720716                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                     | ETA:  10:29:21\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.014 \tlearn_time: 0.020 \tsum: 35245.862 \tsize: 720720                        \n",
      "Episode 36\tTotal score (averaged over agents) this episode: 37.69549915743992\n",
      "\n",
      "Environment solved in 36 episodes!\tAverage Score: 31.07\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.013 \tlearn_time: 0.023 \tsum: 36326.096 \tsize: 740737                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                     | ETA:  10:28:33\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.012 \tlearn_time: 0.019 \tsum: 36317.717 \tsize: 740740                        \n",
      "Episode 37\tTotal score (averaged over agents) this episode: 36.019499194901435\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.010 \tupdate_time: 0.012 \tlearn_time: 0.019 \tsum: 37454.017 \tsize: 760754                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                     | ETA:  10:27:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.011 \tupdate_time: 0.017 \tlearn_time: 0.023 \tsum: 37432.075 \tsize: 760760                        \n",
      "Episode 38\tTotal score (averaged over agents) this episode: 35.51849920609966\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.014 \tlearn_time: 0.021 \tsum: 38485.371 \tsize: 780776                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                     | ETA:  10:26:58\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.013 \tlearn_time: 0.019 \tsum: 38469.258 \tsize: 780780                        \n",
      "Episode 39\tTotal score (averaged over agents) this episode: 35.9669991960749\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.010 \tupdate_time: 0.014 \tlearn_time: 0.020 \tsum: 39645.053 \tsize: 800798                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                     | ETA:  10:26:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.009 \tupdate_time: 0.016 \tlearn_time: 0.025 \tsum: 39635.723 \tsize: 800800                        \n",
      "Episode 40\tTotal score (averaged over agents) this episode: 36.28749918891117\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.012 \tupdate_time: 0.018 \tlearn_time: 0.033 \tsum: 40760.219 \tsize: 820819                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                     | ETA:  10:27:01\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.010 \tupdate_time: 0.016 \tlearn_time: 0.042 \tsum: 40757.202 \tsize: 820820                        \n",
      "Episode 41\tTotal score (averaged over agents) this episode: 36.68349918005988\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.010 \tupdate_time: 0.016 \tlearn_time: 0.023 \tsum: 41650.607 \tsize: 840839                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  14% |#####                                     | ETA:  10:27:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.009 \tupdate_time: 0.013 \tlearn_time: 0.021 \tsum: 41648.189 \tsize: 840840                        \n",
      "Episode 42\tTotal score (averaged over agents) this episode: 37.61399915926158\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.009 \tupdate_time: 0.013 \tlearn_time: 0.026 \tsum: 42852.099 \tsize: 860859                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  14% |######                                    | ETA:  10:27:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.007 \tupdate_time: 0.014 \tlearn_time: 0.023 \tsum: 42850.112 \tsize: 860860                        \n",
      "Episode 43\tTotal score (averaged over agents) this episode: 37.89149915305897\n",
      "\n",
      "Environment solved in 43 episodes!\tAverage Score: 35.29\n",
      "t_step: 1000 \tafter first sample, sample_time: 0.011 \tupdate_time: 0.017 \tlearn_time: 0.023 \tsum: 43828.108 \tsize: 880879                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  14% |######                                    | ETA:  10:27:54\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_step: 1000 \tafter first sample, sample_time: 0.008 \tupdate_time: 0.014 \tlearn_time: 0.022 \tsum: 43826.504 \tsize: 880880                        \n",
      "Episode 44\tTotal score (averaged over agents) this episode: 35.95649919630959\n",
      "t_step: 670 \tafter first sample, sample_time: 0.011 \tupdate_time: 0.015 \tlearn_time: 0.039 \tsum: 44572.631 \tsize: 894300                        "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-61d09706512c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prioritized training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-895bf945d71e>\u001b[0m in \u001b[0;36mLearn\u001b[0;34m(env, n_episodes, max_t, target_score, actual_target_score)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_agent_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;31m# send all actions to tne environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m         \u001b[0;31m# get next state (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m                         \u001b[0;31m# get reward (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prioritized training\n",
    "prioritized_learn = True\n",
    "scores = Learn(env, 300, 10000, 0.0, 30.0, prioritized_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 33 action size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  17:07:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tTotal score (averaged over agents) this episode: 0.5969999866560102\n",
      "\n",
      "Environment solved in 1 episodes!\tAverage Score: 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  16:16:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2\tTotal score (averaged over agents) this episode: 0.6334999858401715\n",
      "\n",
      "Environment solved in 2 episodes!\tAverage Score: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   1% |                                          | ETA:  16:14:53\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3\tTotal score (averaged over agents) this episode: 1.0124999773688614\n",
      "\n",
      "Environment solved in 3 episodes!\tAverage Score: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   1% |                                          | ETA:  16:36:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4\tTotal score (averaged over agents) this episode: 1.4824999668635428\n",
      "\n",
      "Environment solved in 4 episodes!\tAverage Score: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   1% |                                          | ETA:  16:31:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5\tTotal score (averaged over agents) this episode: 1.765499960538\n",
      "\n",
      "Environment solved in 5 episodes!\tAverage Score: 1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |                                          | ETA:  16:27:24\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6\tTotal score (averaged over agents) this episode: 2.2264999502338467\n",
      "\n",
      "Environment solved in 6 episodes!\tAverage Score: 1.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |                                          | ETA:  16:34:56\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7\tTotal score (averaged over agents) this episode: 2.4389999454841016\n",
      "\n",
      "Environment solved in 7 episodes!\tAverage Score: 1.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |#                                         | ETA:  16:32:21\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8\tTotal score (averaged over agents) this episode: 3.768999915756285\n",
      "\n",
      "Environment solved in 8 episodes!\tAverage Score: 1.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                         | ETA:  16:27:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9\tTotal score (averaged over agents) this episode: 3.1969999285414814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                         | ETA:  16:17:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tTotal score (averaged over agents) this episode: 2.5074999439530075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                         | ETA:  16:13:42\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11\tTotal score (averaged over agents) this episode: 2.9969999330118298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                         | ETA:  16:13:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12\tTotal score (averaged over agents) this episode: 3.101999930664897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                         | ETA:  16:08:24\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13\tTotal score (averaged over agents) this episode: 3.263499927055091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                         | ETA:  16:05:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14\tTotal score (averaged over agents) this episode: 3.8519999139010905\n",
      "\n",
      "Environment solved in 14 episodes!\tAverage Score: 2.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   5% |##                                        | ETA:  16:10:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15\tTotal score (averaged over agents) this episode: 4.766999893449247\n",
      "\n",
      "Environment solved in 15 episodes!\tAverage Score: 2.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   5% |##                                        | ETA:  16:18:19\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16\tTotal score (averaged over agents) this episode: 5.023999887704849\n",
      "\n",
      "Environment solved in 16 episodes!\tAverage Score: 2.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   5% |##                                        | ETA:  16:20:09\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17\tTotal score (averaged over agents) this episode: 5.53749987622723\n",
      "\n",
      "Environment solved in 17 episodes!\tAverage Score: 2.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                        | ETA:  16:17:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18\tTotal score (averaged over agents) this episode: 7.0034998434595765\n",
      "\n",
      "Environment solved in 18 episodes!\tAverage Score: 3.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                        | ETA:  16:14:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19\tTotal score (averaged over agents) this episode: 6.449499855842442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                        | ETA:  16:12:49\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tTotal score (averaged over agents) this episode: 7.763999826461077\n",
      "\n",
      "Environment solved in 20 episodes!\tAverage Score: 3.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |##                                        | ETA:  16:13:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21\tTotal score (averaged over agents) this episode: 7.745499826874584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |###                                       | ETA:  16:13:27\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22\tTotal score (averaged over agents) this episode: 7.492499832529575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |###                                       | ETA:  16:10:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23\tTotal score (averaged over agents) this episode: 6.854499846789986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                       | ETA:  16:07:35\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24\tTotal score (averaged over agents) this episode: 7.843499824684113\n",
      "\n",
      "Environment solved in 24 episodes!\tAverage Score: 4.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                       | ETA:  16:06:35\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25\tTotal score (averaged over agents) this episode: 8.717999805137515\n",
      "\n",
      "Environment solved in 25 episodes!\tAverage Score: 5.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                       | ETA:  16:11:25\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26\tTotal score (averaged over agents) this episode: 9.204999794252217\n",
      "\n",
      "Environment solved in 26 episodes!\tAverage Score: 5.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |###                                       | ETA:  16:12:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27\tTotal score (averaged over agents) this episode: 9.500499787647277\n",
      "\n",
      "Environment solved in 27 episodes!\tAverage Score: 5.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |###                                       | ETA:  16:11:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28\tTotal score (averaged over agents) this episode: 9.043499797862022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |####                                      | ETA:  16:12:41\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29\tTotal score (averaged over agents) this episode: 8.712999805249273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |####                                      | ETA:  16:11:24\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30\tTotal score (averaged over agents) this episode: 8.99599979892373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |####                                      | ETA:  16:13:51\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31\tTotal score (averaged over agents) this episode: 9.817499780561775\n",
      "\n",
      "Environment solved in 31 episodes!\tAverage Score: 7.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |####                                      | ETA:  16:17:15\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 32\tTotal score (averaged over agents) this episode: 8.47149981064722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  11% |####                                      | ETA:  16:15:33\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33\tTotal score (averaged over agents) this episode: 10.238499771151691\n",
      "\n",
      "Environment solved in 33 episodes!\tAverage Score: 7.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  11% |####                                      | ETA:  16:14:17\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 34\tTotal score (averaged over agents) this episode: 11.491999743133784\n",
      "\n",
      "Environment solved in 34 episodes!\tAverage Score: 8.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  11% |####                                      | ETA:  16:12:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35\tTotal score (averaged over agents) this episode: 11.25549974841997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                     | ETA:  16:13:56\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 36\tTotal score (averaged over agents) this episode: 10.93449975559488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                     | ETA:  16:16:05\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37\tTotal score (averaged over agents) this episode: 13.38549970081076\n",
      "\n",
      "Environment solved in 37 episodes!\tAverage Score: 9.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                     | ETA:  16:14:25\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 38\tTotal score (averaged over agents) this episode: 12.335999724268913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                     | ETA:  16:12:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39\tTotal score (averaged over agents) this episode: 12.091999729722739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                     | ETA:  16:13:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40\tTotal score (averaged over agents) this episode: 12.813999713584781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                     | ETA:  16:13:36\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 41\tTotal score (averaged over agents) this episode: 12.144499728549272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  14% |#####                                     | ETA:  16:18:15\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42\tTotal score (averaged over agents) this episode: 12.192499727476388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  14% |######                                    | ETA:  16:18:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 43\tTotal score (averaged over agents) this episode: 12.321499724593014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  14% |######                                    | ETA:  16:18:48\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44\tTotal score (averaged over agents) this episode: 12.665999716892838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  15% |######                                    | ETA:  16:19:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 45\tTotal score (averaged over agents) this episode: 13.589499696251005\n",
      "\n",
      "Environment solved in 45 episodes!\tAverage Score: 11.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  15% |######                                    | ETA:  16:19:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46\tTotal score (averaged over agents) this episode: 13.329499702062458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  15% |######                                    | ETA:  16:22:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 47\tTotal score (averaged over agents) this episode: 14.674499671999365\n",
      "\n",
      "Environment solved in 47 episodes!\tAverage Score: 11.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  16% |######                                    | ETA:  16:23:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 48\tTotal score (averaged over agents) this episode: 14.634999672882259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  16% |######                                    | ETA:  16:21:21\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 49\tTotal score (averaged over agents) this episode: 14.210999682359397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  16% |#######                                   | ETA:  16:22:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\tTotal score (averaged over agents) this episode: 15.300999657995998\n",
      "\n",
      "Environment solved in 50 episodes!\tAverage Score: 12.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  17% |#######                                   | ETA:  16:22:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 51\tTotal score (averaged over agents) this episode: 12.985499709751457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  17% |#######                                   | ETA:  16:23:49\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 52\tTotal score (averaged over agents) this episode: 15.603499651234596\n",
      "\n",
      "Environment solved in 52 episodes!\tAverage Score: 12.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  17% |#######                                   | ETA:  16:23:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 53\tTotal score (averaged over agents) this episode: 15.886499644909055\n",
      "\n",
      "Environment solved in 53 episodes!\tAverage Score: 13.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  18% |#######                                   | ETA:  16:23:53\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 54\tTotal score (averaged over agents) this episode: 15.849999645724893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  18% |#######                                   | ETA:  16:23:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 55\tTotal score (averaged over agents) this episode: 16.40649963328615\n",
      "\n",
      "Environment solved in 55 episodes!\tAverage Score: 13.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  18% |#######                                   | ETA:  16:24:42\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 56\tTotal score (averaged over agents) this episode: 17.31849961290136\n",
      "\n",
      "Environment solved in 56 episodes!\tAverage Score: 13.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  19% |#######                                   | ETA:  16:22:33\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 57\tTotal score (averaged over agents) this episode: 17.275999613851308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  19% |########                                  | ETA:  16:19:32\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 58\tTotal score (averaged over agents) this episode: 17.622999606095256\n",
      "\n",
      "Environment solved in 58 episodes!\tAverage Score: 14.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  19% |########                                  | ETA:  16:19:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 59\tTotal score (averaged over agents) this episode: 16.59449962908402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                  | ETA:  16:19:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60\tTotal score (averaged over agents) this episode: 14.647499672602862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                  | ETA:  16:19:31\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 61\tTotal score (averaged over agents) this episode: 15.583999651670457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                  | ETA:  16:16:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 62\tTotal score (averaged over agents) this episode: 14.270499681029468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  21% |########                                  | ETA:  16:14:30\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 63\tTotal score (averaged over agents) this episode: 16.675999627262353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  21% |########                                  | ETA:  16:11:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 64\tTotal score (averaged over agents) this episode: 18.15899959411472\n",
      "\n",
      "Environment solved in 64 episodes!\tAverage Score: 15.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  21% |#########                                 | ETA:  16:09:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 65\tTotal score (averaged over agents) this episode: 19.722499559167773\n",
      "\n",
      "Environment solved in 65 episodes!\tAverage Score: 15.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  22% |#########                                 | ETA:  16:05:47\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 66\tTotal score (averaged over agents) this episode: 18.629499583598225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  22% |#########                                 | ETA:  16:01:55\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 67\tTotal score (averaged over agents) this episode: 20.863499533664434\n",
      "\n",
      "Environment solved in 67 episodes!\tAverage Score: 16.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  22% |#########                                 | ETA:  16:00:22\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 68\tTotal score (averaged over agents) this episode: 20.992999530769886\n",
      "\n",
      "Environment solved in 68 episodes!\tAverage Score: 16.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  23% |#########                                 | ETA:  15:56:56\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 69\tTotal score (averaged over agents) this episode: 22.997499485965818\n",
      "\n",
      "Environment solved in 69 episodes!\tAverage Score: 17.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  23% |#########                                 | ETA:  15:53:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 70\tTotal score (averaged over agents) this episode: 24.406499454472215\n",
      "\n",
      "Environment solved in 70 episodes!\tAverage Score: 17.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  23% |#########                                 | ETA:  15:49:51\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 71\tTotal score (averaged over agents) this episode: 23.108499483484774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  24% |##########                                | ETA:  15:46:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 72\tTotal score (averaged over agents) this episode: 23.248999480344356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  24% |##########                                | ETA:  15:43:26\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 73\tTotal score (averaged over agents) this episode: 23.187499481718987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  24% |##########                                | ETA:  15:39:25\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 74\tTotal score (averaged over agents) this episode: 24.137999460473658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  25% |##########                                | ETA:  15:35:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 75\tTotal score (averaged over agents) this episode: 25.104999438859522\n",
      "\n",
      "Environment solved in 75 episodes!\tAverage Score: 19.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  25% |##########                                | ETA:  15:31:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 76\tTotal score (averaged over agents) this episode: 27.55049938419834\n",
      "\n",
      "Environment solved in 76 episodes!\tAverage Score: 20.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  25% |##########                                | ETA:  15:27:01\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 77\tTotal score (averaged over agents) this episode: 29.332499344367534\n",
      "\n",
      "Environment solved in 77 episodes!\tAverage Score: 20.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  26% |##########                                | ETA:  15:23:04\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 78\tTotal score (averaged over agents) this episode: 31.74199929051101\n",
      "\n",
      "Environment solved in 78 episodes!\tAverage Score: 21.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  26% |###########                               | ETA:  15:19:01\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 79\tTotal score (averaged over agents) this episode: 32.509999273344874\n",
      "\n",
      "Environment solved in 79 episodes!\tAverage Score: 22.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  26% |###########                               | ETA:  15:14:53\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80\tTotal score (averaged over agents) this episode: 31.997499284800142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  27% |###########                               | ETA:  15:10:44\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 81\tTotal score (averaged over agents) this episode: 31.128499304223805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  27% |###########                               | ETA:  15:06:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 82\tTotal score (averaged over agents) this episode: 30.159499325882642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  27% |###########                               | ETA:  15:02:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 83\tTotal score (averaged over agents) this episode: 31.877999287471177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  28% |###########                               | ETA:  14:58:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 84\tTotal score (averaged over agents) this episode: 33.78249924490228\n",
      "\n",
      "Environment solved in 84 episodes!\tAverage Score: 26.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  28% |###########                               | ETA:  14:54:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 85\tTotal score (averaged over agents) this episode: 38.14149914747104\n",
      "\n",
      "Environment solved in 85 episodes!\tAverage Score: 27.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  28% |############                              | ETA:  14:49:53\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 86\tTotal score (averaged over agents) this episode: 37.09899917077273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  29% |############                              | ETA:  14:45:48\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 87\tTotal score (averaged over agents) this episode: 37.968499151337895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  29% |############                              | ETA:  14:41:30\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 88\tTotal score (averaged over agents) this episode: 39.139499125164\n",
      "\n",
      "Environment solved in 88 episodes!\tAverage Score: 29.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  29% |############                              | ETA:  14:37:16\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 89\tTotal score (averaged over agents) this episode: 38.44449914069846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  30% |############                              | ETA:  14:33:07\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 90\tTotal score (averaged over agents) this episode: 37.57949916003272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  30% |############                              | ETA:  14:29:09\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 91\tTotal score (averaged over agents) this episode: 38.48849913971499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  30% |############                              | ETA:  14:25:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 92\tTotal score (averaged over agents) this episode: 38.78049913318828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  31% |#############                             | ETA:  14:21:01\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 93\tTotal score (averaged over agents) this episode: 37.67249915795401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  31% |#############                             | ETA:  14:17:01\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 94\tTotal score (averaged over agents) this episode: 37.230999167822304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  31% |#############                             | ETA:  14:12:54\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 95\tTotal score (averaged over agents) this episode: 37.305999166145924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |#############                             | ETA:  14:09:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 96\tTotal score (averaged over agents) this episode: 37.370499164704235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |#############                             | ETA:  14:05:10\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 97\tTotal score (averaged over agents) this episode: 37.372999164648355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |#############                             | ETA:  14:01:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 98\tTotal score (averaged over agents) this episode: 38.32449914338067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  33% |#############                             | ETA:  13:57:40\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99\tTotal score (averaged over agents) this episode: 38.09999914839864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  33% |##############                            | ETA:  13:53:41\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tTotal score (averaged over agents) this episode: 37.88249915326014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100% |###########################################| Time: 7:01:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 101\tTotal score (averaged over agents) this episode: 38.50299913939089\n",
      "training done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "prioritized_learn = False\n",
    "scores = Learn(env, 300, 10000, 0.0, 30.0, prioritized_learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('drlnd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a2a5acb46a67fdc4db5f9e567de32cf722fea3c072c2c9c5a0219048153f98c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
